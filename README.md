# EnronMLProject
by Chittaranjan Chandwani
# Overview
In the year 1985, with the merger of two companies, Huston Natural Gas (HNG) and InterNorth, Enron was born. It was regarded as one of the best energy and natural gas companies in the world. And for six consecutive years was named "America's Most Innovative Company" by Fortune Magazine. Kenneth Lee Lay was the chairman and CEO of the company. Later, Jeffery Skilling was appointed as the new CEO while Ken Lay maintained his position as the chairman.
However, what looked like a dream company from the outside was actually crumbling under the weight of billions of dollars of debts and failed project from within. These debts and losses were skillfully, willfully and systematically swept under the rug through accounting and auditing frauds over a sustained period of time. Eventually, Enron filled for bankruptcy in the year 2001 wiping out $78 billion in stock market value. It has since been known as the biggest case of corporate fraud in U.S history ever.

Post the federal investigation of Enron Scandal, confidential information regarding the case crept into public domain. This information contained 600,000 emails generated by 158 enron employess and came to be known as the Enron Corpus.

My objective for this project is to use Machine Learning techniques to identify Persons of Interest from the data available from the Enron Corpus. A Person of Interest (POI) in this case is referred as person who was involved in the case.

# Enron Data
The goal of this project, as I mentioned above, is to create model(s) that can effectively predict Person Of Interest (POI) and Non-Person Of Interest (non-POI) from the available dataset. Therefore, to begin there were a few things about the dataset that I wanted to know straight off the bat, like:
Names of all the executives in the dataset
1. Number of total executives
2. Total number of features
3. Total number of POI's
4. Total number of non-POI's

# Data Exploration
Here is a summary of the data:

Total number of data points: 146
Total number of poi: 18
Total number of non-poi: 128
Total number of features: 21

# Missing Values
Every feature (except poi) in the dataset has missing values or 'NaN' Values. I thought it would be useful to visualize the number of 'NaN' values for all the features. Below is a bar chart displaying those values.

# Outlier Removal
I suspected as outliers data points for which at least 85% of the features are either 0 or NaN and the following function, suspectDataPointForWhichMoreThanXpercentFeaturesIsNaN(), suspected the data keys below: 'WHALEY DAVID A', 'WROBEL BRUCE', 'LOCKHART EUGENE E', 'THE TRAVEL AGENCY IN THE PARK', 'GRAMM WENDY L'

Upon manual inspection, I resolve to consider only ‘LOCKHART EUGENE E’ as an outlier because I noticed that every single one of its features were either NaN or 0, hence, can be safely remove.

# Model Validation Selection and Scores
In Model validation a trained model is evaluated using a testing data set. The testing data set is a separate portion of the same data set from which the training set is derived. The main purpose of using the testing data set is to test how well the model will perform when new data points are introduced.

However, while splitting a data set into training and test sets there is a tradeoff where data points are lost from the training set and assigned to the test set. This where cross-validation technique known as k-fold cross-validation comes into picture. By using k-fold cross-validation, the original data set is randomly partitioned into k subsamples and one is left out in each iteration. The performance of the test set is computed by taking an average of the total number of iterations.

If cross validation is not performed on a data set, if a data set is not processed into a training and test sets, a machine learning algorithm could become a victim of over fitting. Because the algorithm will be built by fitting the entire data set, it won't learn anything new about the data and won't be able to adapt itself to new information. As a result, the algorithm will perform counter productively whenever it gets exposed to new data points.

Below is a list of five algorithms I tried with an objective to classify which employees are more likely to be Persons Of Interest:

Gaussian Naive Bayes
Support Vector Classifier
Decision Tree Classifier
SVM Grid Search Classifier
Adaboost Classifier
I used the train_test_split function to split the data into a 70:30 ratio; 70% for training, 30% for testing. Furthermore, I used average scores of three metrics accuarcy, precision and recall to evaluate the performance of my models. This average was computed based on 20 iterations of each algorithm. Below is quick demo of each of the five algorithms tested with and without new features. These algorithms are used straight out of the box without tuning any parameters. I used two function train_predict_evaluate and score_chart to evaluate the scores of each algorithm and plot a comparison.

Considering the above result, it is obvious that both Support Vector Machine (SVM) and Grid Search (with SVM) produce exactly the same result. I considered Decision Tree of better performance and more suited to analyzing the skewed Enron dataset because in my judgment, it produces the best combination of accuracy, precision and recall for this dataset; justification for my assertion will be provided in later questions.

# Model Tuning
In machine learning, a hyper-parameter is a parameter whose value is set before the learning process begins. The process of setting the values of these parameters for a learning algorithm either by using model defaults, grid search, and random search or Bayesian optimization is called tuning. The tunable parameters of the models can greatly affect their accuracy, without doing this well, the model will not optimally solve the machine learning problem. The more tuned the parameters of an algorithm, the more biased the algorithm will behave relative to the training and test dataset. This approach can be effective, but it can also lead to a very fragile models that over fit the test data, hence, performing poorly in practice.

My final algorithm was decision tree, studying the arrays of tunable parameters for decision; I found no strategic advantage or reason to modify the default, so I used the default parameters without further tuning. However, for other algorithm like SVM, I experimented with grid search using the following parameters set: {'kernel': ('linear', 'rbf'), 'C': [1, 10]} With adaboost, the parameters I tuned were : n_estimators=1000, random_state=202, learning_rate=1.0, algorithm="SAMME.R" Generally, the approaches I used for tuning the parameters divides into two: an automatic approach using GridSearchCV and manual intelligent search approach.

# Conclusion
Considering my experimental result in table 2, in all the five algorithms that were experimented, Naïve Bayes gives the highest and best recall value; that is, the highest probability of getting a person of interest. However, I considered Decision Tree a better algorithm because, it gives the most stable average result for accuracy, precision and recall for all the three experiments and feature sets. Although, Naive Bayes algorithm gives the highest reall, it also seems heavily flawed with bias relative to Decision Tree algorithm.
